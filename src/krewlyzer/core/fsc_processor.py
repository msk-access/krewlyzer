"""
FSC (Fragment Size Coverage) processor.

Processes GC-weighted fragment counts into ML-ready features per the implementation plan:
- Raw GC-weighted counts per channel (from Rust backend)
- PoN log2 ratios: log2(channel / PoN_mean)
- Reliability scores: 1 / (PoN_variance + k)

NO z-scores - the plan specifies GC-weighting (Rust) + PoN log-ratios (Python).
"""

from pathlib import Path
import pandas as pd
import numpy as np
import logging

logger = logging.getLogger("core.fsc_processor")

# 5 biologically-meaningful channels matching Rust backend
CHANNELS = ['ultra_short', 'core_short', 'mono_nucl', 'di_nucl', 'long']

# Regularization constant for reliability calculation
RELIABILITY_K = 0.01


# ============================================================================
# GC CORRECTION FACTOR LOADING
# ============================================================================
#
# For gene-level FSC in panel mode, we apply GC correction weights to account
# for capture bias. Different GC content leads to different hybridization
# efficiency and PCR amplification rates.
#
# The correction factors are loaded from .correction_factors.ontarget.tsv
# generated by the extract step with --target-regions.
# ============================================================================

def load_correction_factors(factor_path: Path) -> dict:
    """
    Load GC correction factors into a fast lookup table.
    
    The factors are indexed by (length_bin, gc_percent) for O(1) lookup.
    Length bins match the Rust backend: bin = (fragment_length - 60) // 5
    
    Args:
        factor_path: Path to correction_factors.ontarget.tsv or .tsv
        
    Returns:
        Dict[(len_bin, gc_pct), factor] for fast lookup
        Empty dict if loading fails (falls back to unweighted counting)
    """
    factors = {}
    
    try:
        # Detect format from extension
        sep = ',' if str(factor_path).endswith('.csv') else '\t'
        df = pd.read_csv(factor_path, sep=sep)
        
        # Expected columns: length_bin (or similar), gc_percent, factor
        # Find the relevant columns
        len_col = None
        gc_col = None
        factor_col = None
        
        for col in df.columns:
            col_lower = col.lower()
            if 'length' in col_lower and 'bin' in col_lower:
                len_col = col
            elif 'gc' in col_lower and ('percent' in col_lower or 'pct' in col_lower):
                gc_col = col
            elif 'factor' in col_lower or 'correction' in col_lower:
                factor_col = col
        
        if not all([len_col, gc_col, factor_col]):
            logger.warning(f"Correction factors file missing expected columns: {list(df.columns)}")
            return factors
        
        # Build lookup table
        # Note: length_bin column may contain length_bin_min values (e.g., 75, 80, ...)
        # or bin indices (e.g., 3, 4, ...). We detect which format and convert.
        is_raw_lengths = len_col.lower().endswith('_min') or df[len_col].min() >= 60
        
        for _, row in df.iterrows():
            try:
                len_val = int(row[len_col])
                gc_pct = int(row[gc_col])
                factor = float(row[factor_col])
                
                # Convert raw length to bin index if needed
                # Bin formula matches Rust: (length - 60) // 5
                if is_raw_lengths:
                    len_bin = (len_val - 60) // 5
                else:
                    len_bin = len_val
                
                # Validate bin index (0-67 for lengths 60-399)
                if len_bin < 0 or len_bin > 67:
                    continue
                
                # Validate factor (should be positive, not too extreme)
                if 0.01 < factor < 100:
                    factors[(len_bin, gc_pct)] = factor
            except (ValueError, TypeError):
                continue
        
        logger.info(f"Loaded {len(factors)} GC correction factors from {factor_path.name}")
        
    except FileNotFoundError:
        logger.warning(f"Correction factors file not found: {factor_path}")
    except Exception as e:
        logger.warning(f"Failed to load correction factors from {factor_path}: {e}")
    
    return factors


def _get_gc_weight(frag_len: int, gc_frac: float, correction_factors: dict) -> float:
    """
    Get the GC correction weight for a fragment.
    
    Args:
        frag_len: Fragment length in bp
        gc_frac: GC fraction (0.0 to 1.0)
        correction_factors: Lookup table from load_correction_factors()
        
    Returns:
        Correction weight (default 1.0 if not found)
    """
    if not correction_factors:
        return 1.0
    
    # Calculate length bin (matches Rust: (len - 60) / 5, clamped to 0-67)
    safe_len = max(60, min(399, frag_len))
    len_bin = (safe_len - 60) // 5
    
    # Convert GC fraction to percentage (0-100)
    gc_pct = int(round(gc_frac * 100))
    gc_pct = max(0, min(100, gc_pct))
    
    return correction_factors.get((len_bin, gc_pct), 1.0)


def process_fsc(
    counts_df: pd.DataFrame,
    output_path: Path,
    windows: int = 100000,
    continue_n: int = 50,
    pon = None
) -> Path:
    """
    Process GC-weighted fragment counts into FSC ML features.
    
    Per the implementation plan, FSC normalization order is:
    1. GC-weight (Rust) - done upstream
    2. PoN log-ratio (Python) - done here
    
    Output columns:
    - chrom, start, end: Bin coordinates
    - ultra_short, core_short, mono_nucl, di_nucl, long: GC-weighted counts
    - total: All fragments
    - *_log2: log2(channel / PoN_mean) when PoN provided
    - *_reliability: 1 / (PoN_variance + k) when PoN provided
    
    Args:
        counts_df: DataFrame from Rust with [chrom, start, end, ultra_short, 
                   core_short, mono_nucl, di_nucl, long, total, mean_gc]
        output_path: Path to write FSC.tsv output
        windows: Window size in bp (default 100000)
        continue_n: Number of bins to aggregate (default 50)
        pon: Optional PON model for log2 ratio normalization
        
    Returns:
        Path to the output file
    """
    logger.info(f"Processing FSC: {len(counts_df)} bins â†’ {output_path}")
    
    # Validate required columns
    required_cols = ['chrom', 'start', 'end'] + CHANNELS + ['total']
    missing = [c for c in required_cols if c not in counts_df.columns]
    if missing:
        raise ValueError(f"Missing required columns: {missing}")
    
    # Aggregate bins into windows
    results = []
    for chrom, group in counts_df.groupby('chrom', sort=False):
        n_bins = len(group)
        n_windows = n_bins // continue_n
        
        if n_windows == 0:
            continue
        
        trunc_len = n_windows * continue_n
        
        # Aggregate each channel
        channel_sums = {}
        for ch in CHANNELS:
            mat = group[ch].values[:trunc_len].reshape(n_windows, continue_n)
            channel_sums[ch] = mat.sum(axis=1)
        
        totals_mat = group['total'].values[:trunc_len].reshape(n_windows, continue_n)
        channel_sums['total'] = totals_mat.sum(axis=1)
        
        # Get window coordinates from first and last bin in each window
        starts = group['start'].values[:trunc_len].reshape(n_windows, continue_n)[:, 0]
        ends = group['end'].values[:trunc_len].reshape(n_windows, continue_n)[:, -1]
        
        window_df = pd.DataFrame({
            'chrom': chrom,
            'start': starts,
            'end': ends,
            **channel_sums
        })
        results.append(window_df)
    
    if not results:
        logger.warning("No valid windows found for FSC")
        _write_empty_fsc(output_path, pon is not None)
        return output_path
    
    final_df = pd.concat(results, ignore_index=True)
    logger.info(f"Aggregated into {len(final_df)} windows")
    
    # Compute PoN log2 ratios and reliability if model provided
    if pon is not None:
        logger.info("Computing PoN log2 ratios...")
        for ch in CHANNELS:
            mean = pon.get_mean(ch) if hasattr(pon, 'get_mean') else None
            var = pon.get_variance(ch) if hasattr(pon, 'get_variance') else None
            
            if mean and mean > 0:
                # log2(channel / PoN_mean)
                final_df[f'{ch}_log2'] = np.log2(final_df[ch] / mean + 1e-9)
            else:
                final_df[f'{ch}_log2'] = 0.0
            
            if var is not None:
                # reliability = 1 / (PoN_variance + k)
                final_df[f'{ch}_reliability'] = 1.0 / (var + RELIABILITY_K)
            else:
                final_df[f'{ch}_reliability'] = 1.0
    
    # Write output
    _write_fsc_output(final_df, output_path, pon is not None)
    
    logger.info(f"FSC complete: {output_path}")
    return output_path


def _write_empty_fsc(output_path: Path, with_pon: bool):
    """Write empty FSC file with appropriate headers."""
    cols = ['chrom', 'start', 'end'] + CHANNELS + ['total']
    if with_pon:
        cols += [f'{ch}_log2' for ch in CHANNELS]
        cols += [f'{ch}_reliability' for ch in CHANNELS]
    
    pd.DataFrame(columns=cols).to_csv(output_path, sep='\t', index=False)


def _write_fsc_output(df: pd.DataFrame, output_path: Path, with_pon: bool):
    """Write FSC output TSV matching the implementation plan schema."""
    # Determine columns to output
    cols = ['chrom', 'start', 'end'] + CHANNELS + ['total']
    if with_pon:
        cols += [f'{ch}_log2' for ch in CHANNELS]
        cols += [f'{ch}_reliability' for ch in CHANNELS]
    
    # Select and write
    output_df = df[cols].copy()
    output_df.to_csv(output_path, sep='\t', index=False, float_format='%.4f')


def aggregate_by_gene(
    fragments_bed: Path,
    genes: dict,
    output_path: Path,
    pon=None,
    correction_factors: dict = None
) -> Path:
    """
    Aggregate fragment counts by gene for panel-specific FSC.
    
    Instead of genomic windows, this function sums fragment counts
    per gene target regions, producing gene-level FSC features.
    
    When correction_factors is provided, fragments are weighted by their
    GC correction factor to account for capture and PCR bias. This is
    critical for accurate copy number analysis in panel mode.
    
    Args:
        fragments_bed: Path to fragment BED file from extract step
                       Format: chrom, start, end, gc_fraction
        genes: Dict[gene_name, List[Region]] from gene_bed.load_gene_bed()
        output_path: Path to write gene FSC output
        pon: Optional PON model for normalization
        correction_factors: Optional dict[(len_bin, gc_pct) -> factor]
                           Loaded via load_correction_factors()
        
    Returns:
        Path to output file
        
    Output columns:
        - gene: Gene name
        - n_regions: Number of target regions for this gene
        - total_bp: Total basepairs covered
        - ultra_short, core_short, mono_nucl, di_nucl, long: Weighted channel counts
        - total: Total weighted fragments
        - *_ratio: Channel / total ratio
    """
    import gzip
    
    # Log GC correction mode
    gc_mode = "ON (weighted counting)" if correction_factors else "OFF (raw counting)"
    logger.info(f"Aggregating FSC by gene: {len(genes)} genes, GC correction: {gc_mode}")
    
    # Build interval tree for fast region lookup
    # Structure: chrom -> [(start, end, gene), ...]
    gene_intervals = {}
    for gene, regions in genes.items():
        for region in regions:
            chrom = region.chrom
            if chrom not in gene_intervals:
                gene_intervals[chrom] = []
            gene_intervals[chrom].append((region.start, region.end, gene))
    
    # Sort intervals per chromosome
    for chrom in gene_intervals:
        gene_intervals[chrom].sort(key=lambda x: x[0])
    
    # Initialize gene counts as floats for weighted counting
    gene_counts = {gene: {ch: 0.0 for ch in CHANNELS + ['total']} for gene in genes}
    gene_meta = {gene: {'n_regions': len(regions), 'total_bp': sum(r.end - r.start for r in regions)} 
                 for gene, regions in genes.items()}
    
    # Tracking statistics for monitoring
    total_fragments = 0
    assigned_fragments = 0
    gc_missing_count = 0
    total_weight = 0.0
    
    open_func = gzip.open if str(fragments_bed).endswith('.gz') else open
    with open_func(fragments_bed, 'rt') as f:
        for line in f:
            if line.startswith('#'):
                continue
            
            fields = line.strip().split('\t')
            if len(fields) < 3:
                continue
            
            total_fragments += 1
            chrom = fields[0]
            try:
                start = int(fields[1])
                end = int(fields[2])
            except ValueError:
                continue
            
            # Parse GC fraction from column 4 (if available)
            # BED format: chrom, start, end, gc_fraction
            gc_frac = 0.5  # Default to neutral GC
            if len(fields) >= 4 and correction_factors:
                try:
                    gc_frac = float(fields[3])
                except ValueError:
                    gc_missing_count += 1
            
            # Determine size bin
            frag_len = end - start
            if frag_len < 65:
                continue  # Too short
            elif frag_len < 100:
                size_bin = 'ultra_short'
            elif frag_len < 150:
                size_bin = 'core_short'
            elif frag_len < 260:
                size_bin = 'mono_nucl'
            elif frag_len < 400:
                size_bin = 'di_nucl'
            else:
                size_bin = 'long'
            
            # Calculate weight using GC correction factors
            weight = _get_gc_weight(frag_len, gc_frac, correction_factors)
            
            # Find overlapping gene regions
            if chrom not in gene_intervals:
                continue
            
            frag_mid = (start + end) // 2
            for reg_start, reg_end, gene in gene_intervals[chrom]:
                if reg_start > frag_mid:
                    break  # Past this fragment
                if reg_start <= frag_mid < reg_end:
                    # Apply weighted counting (key fix for GC bias)
                    gene_counts[gene][size_bin] += weight
                    gene_counts[gene]['total'] += weight
                    assigned_fragments += 1
                    total_weight += weight
                    break  # Assign to first matching gene only
    
    # Log processing statistics with GC correction info
    logger.info(f"Processed {total_fragments} fragments, {assigned_fragments} assigned to genes")
    if correction_factors:
        avg_weight = total_weight / assigned_fragments if assigned_fragments > 0 else 1.0
        logger.info(f"  GC correction: avg_weight={avg_weight:.3f}, missing_gc={gc_missing_count}")
        if avg_weight < 0.5 or avg_weight > 2.0:
            logger.warning(f"  Unusual average weight {avg_weight:.3f} - check correction factors")
    
    # Build output DataFrame
    rows = []
    for gene in sorted(genes.keys()):
        row = {
            'gene': gene,
            'n_regions': gene_meta[gene]['n_regions'],
            'total_bp': gene_meta[gene]['total_bp'],
            **gene_counts[gene]
        }
        # Calculate channel ratios
        total = gene_counts[gene]['total']
        for ch in CHANNELS:
            row[f'{ch}_ratio'] = gene_counts[gene][ch] / total if total > 0 else 0.0
        rows.append(row)
    
    df = pd.DataFrame(rows)
    
    # Add PoN normalization if available
    if pon is not None:
        logger.info("Computing PoN log2 ratios for gene FSC...")
        for ch in CHANNELS:
            mean = pon.get_mean(f'gene_{ch}') if hasattr(pon, 'get_mean') else None
            if mean and mean > 0:
                df[f'{ch}_log2'] = np.log2(df[ch] / mean + 1e-9)
            else:
                df[f'{ch}_log2'] = 0.0
    
    # Write output
    df.to_csv(output_path, sep='\t', index=False, float_format='%.4f')
    logger.info(f"Gene FSC complete: {output_path} ({len(df)} genes)")
    
    return output_path

